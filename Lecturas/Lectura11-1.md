# **Critica: Deep Learning based Recommender System: A Survey and New Perspectives (from 3.5 to end)**

_**Impresión del Paper (Personal):**_ 

>Una extensa revisión de sistemas recomendadores, se centra en la aplicación cualitativa y teórica de las diferentes técnicas, es una descripción desde mi punto de vista muy centrada en público intermedio, y sirve como base para definir el estado del arte hasta la fecha de estudio en el uso de Deep Learning en Sistemas recomendadores (o en algunas de sus fases), además definen adecuadamente con cierto nivel de profundidad matemática (suficiente para entender y posteriormente investigar y aplicar). De pronto la extensión se hubiera podido reducir en las partes introductorias y explicaciones repetitivas.

```[Deep learning - estado del arte en recomendación: (buena subdivisión del estudio) Como dijo el carnicero vamos por partes].```

Dando continuidad a la lectura, la revisión bibliográfica habla de **múltiples temáticas** (_RNN, RBM, NA, NAR [Neural AutoRegressive], DRL [Deep Reinforcement Learning],  AN [Adversarial Network] y modelos híbridos_), resaltando de estos sus características y modelos de aplicación, finalizando con avances y trabajo futuro (_al final una conclusión simple de un trabajo tan amplio y completo_).

Como se realizó en la anterior lectura daré **explicación y compresión a algunos métodos explicados** (_debido a su extensión_), de la **RNN** (_Recurrent Neural Networks_) se identifica que son muy **buenas para las secuencias** de datos, permite el **manejo de dinámicas de tipo temporal**, tanto en interacciones y patrones secuenciales (_usuarios-items, entre usuarios, y posibles interacciones de diferentes ordenes por semejanza o lazos de conexión_).

Se puede realizar en dos modelos **con y sin identificación de usuario** (_usando sesiones, conexiones e interacciones_), la primera no exige el inicio de sesión haciendo **uso de cookies**, trae problemas debido a la **escasez de datos de entrenamiento** (_pero ayuda a dar un arranque frio, problemática en mucho sistema_), por medio de binarización definen al usuario con **1** si esta activo o **0** si no. La salida que se busca en recomendación es la **probabilidad del siguiente elemento**, además de la inclusión de mini-lotes (_para reducir la cantidad de datos observados_).

**GRU4REC** ofrece el uso de esta temática basado en la ecuación ![GRU4REC](https://latex.codecogs.com/gif.latex?%5CL_s%20%3D%5Cfrac%7B1%7D%7BS%7D%5Csum_%7Bj%3D1%7D%5E%7BS%7D%5Csigma%20%5Cleft%20%28%20%5Chat%7Br%7D_%7Bsj%7D-%5Chat%7Br%7D_%7Bsi%7D%20%5Cright%20%29&plus;%5Csigma%20%5Cleft%20%28%20%5Chat%7Br%7D_%7Bsj%7D%5E%7B2%7D%20%5Cright%20%29) siendo la fusión de perdida para el Ranking, **_S_** es el tamaño de la muestra _r(si)_ y _r(sj)_ son valores negativos del item _i_ y positivos del _j_,  _**σ**_ función logística sigmoide (_el ultimo termino es solo regularización de salida_), donde su entrada son las **sesiones actuales** con estado sesiones 1 a N (_N= items_), como ya indique usan lo expuesto en el párrafo anterior (_binarización_). Hay que resaltar que se puede incluir **información adicional** o secundaria (_contexto, de entradas y  salidas, vectores de identidad, imágenes texto…_).

Las **RBM** (_Restricted Boltzmann Machines_) debido a su restricción binaria, hace uso de vectores de calificación (_EJ: [0,0,0,1,0]=4 en rating_), permite mezclar con **CF** (_Collaborative Filtering_) en esta temática solo indican algunas referencias y avances de lo logrado (_no se profundiza_), por otro lado las **NA** (_Neural Attention_) se centra en el uso de **entradas visuales** (_usadas en procesamiento de imagen y visión por computadora_), permite la **reducción de datos ruidosos** (_evita el uso de información adicional innecesaria_), permite bajo su arquitectura la mezcla de otras técnicas (_resaltan las MLP, CNN y RNN_), con las CNN **mejora aspectos informativos** y la RNN **datos largos y ruidoso** (_usando puntajes de atención_), esta NA usa la **atención estándar** (_Vector de contexto para aprender la atención, muestran 10 referencias_) y la **atención conjunta** (_usa dos vectores de secuencias para aprendizaje, muestran 5 referencias_). El modelo **Co-Attention** (_muestran AttRec_), mejora la recomendación secuencial, usando la auto atención (_como intenciones a corto plazo_) y el aprendizaje métrico (_con uso de usuarios e items más activos, se aprecia que es un campo abierto_), posteriormente **permite integrar la información visual y textual**.

De las **secciones 3.8 a la 3.10** se toca superficialmente las temáticas (_AN, Autorregresivo y refuerzos_), lo cual deja percibir el **desperdicio de múltiples referencias captadas por el estudio**, posterior a ello se toca el tema de Modelos Híbridos, esto es **posible a las redes y su arquitectura** (_uso de bloques y diferentes tipos de objetivos de entradas y salidas_), además de la posibilidad de **múltiples combinaciones** (_usando parámetros regularización en las Redes que se deseen mezclar_).

A nivel de conclusiones  y trabajo futuro deja mucho que desear, pues **no se cuenta con un análisis** cuantitativo, cualitativo (_al final del trabajo_), enfoque bajo contexto, un análisis geográfico permitiría identifica quienes están trabajando con fuerza en ello (_permite análisis de alianzas y colaboraciones_)…,  tampoco se realizó la explicación clara del esquema de **clasificación de las referencias** (_varias cuentan con múltiples participaciones en diferentes espacios, aqui expuestos_), nos deja una simple y llana **conclusión VACIA** (_este trabajo daba para muchísimo más_). 

* > **PD**: La lectura es **bastante extensa**, pese a ser una temática muy entretenida es bastante **difícil de digerir**, pero si se realiza para **obtener bagaje y conocimiento superficial** es excelente (_además de ser un gran estado del arte_) de allí la frase de inicial, es importante **solo tomar las partes que nos servirán o nos son más interesantes** (_concepto ambiguo del lector_).
